WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized distributed mode...
Start training
Load PResNet50 state_dict
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
loading annotations into memory...
Done (t=11.43s)
creating index...
index created!
loading annotations into memory...
Done (t=0.37s)
creating index...
index created!
number of params: 42862860
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch: [0]  [    0/14786]  eta: 8:18:30  lr: 0.000010  loss: 44.2697 (44.2697)  loss_bbox: 1.5648 (1.5648)  loss_bbox_aux_0: 1.6119 (1.6119)  loss_bbox_aux_1: 1.5946 (1.5946)  loss_bbox_aux_2: 1.5447 (1.5447)  loss_bbox_aux_3: 1.5455 (1.5455)  loss_bbox_aux_4: 1.5546 (1.5546)  loss_bbox_aux_5: 1.5922 (1.5922)  loss_bbox_dn_0: 0.9693 (0.9693)  loss_bbox_dn_1: 0.9693 (0.9693)  loss_bbox_dn_2: 0.9693 (0.9693)  loss_bbox_dn_3: 0.9693 (0.9693)  loss_bbox_dn_4: 0.9693 (0.9693)  loss_bbox_dn_5: 0.9693 (0.9693)  loss_giou: 1.6684 (1.6684)  loss_giou_aux_0: 1.6733 (1.6733)  loss_giou_aux_1: 1.6542 (1.6542)  loss_giou_aux_2: 1.6748 (1.6748)  loss_giou_aux_3: 1.6704 (1.6704)  loss_giou_aux_4: 1.6730 (1.6730)  loss_giou_aux_5: 1.7082 (1.7082)  loss_giou_dn_0: 1.3386 (1.3386)  loss_giou_dn_1: 1.3386 (1.3386)  loss_giou_dn_2: 1.3386 (1.3386)  loss_giou_dn_3: 1.3386 (1.3386)  loss_giou_dn_4: 1.3386 (1.3386)  loss_giou_dn_5: 1.3386 (1.3386)  loss_vfl: 0.3937 (0.3937)  loss_vfl_aux_0: 0.3685 (0.3685)  loss_vfl_aux_1: 0.4064 (0.4064)  loss_vfl_aux_2: 0.3983 (0.3983)  loss_vfl_aux_3: 0.3925 (0.3925)  loss_vfl_aux_4: 0.3979 (0.3979)  loss_vfl_aux_5: 0.3867 (0.3867)  loss_vfl_dn_0: 0.8527 (0.8527)  loss_vfl_dn_1: 0.8065 (0.8065)  loss_vfl_dn_2: 0.8498 (0.8498)  loss_vfl_dn_3: 0.7679 (0.7679)  loss_vfl_dn_4: 0.8568 (0.8568)  loss_vfl_dn_5: 0.8138 (0.8138)  time: 2.0229  data: 0.6115  max mem: 3959
Epoch: [0]  [  100/14786]  eta: 1:12:57  lr: 0.000010  loss: 36.2071 (39.6599)  loss_bbox: 0.8230 (1.0937)  loss_bbox_aux_0: 0.8269 (1.1442)  loss_bbox_aux_1: 0.8266 (1.1268)  loss_bbox_aux_2: 0.8204 (1.1126)  loss_bbox_aux_3: 0.8272 (1.1039)  loss_bbox_aux_4: 0.8236 (1.0985)  loss_bbox_aux_5: 0.8448 (1.1876)  loss_bbox_dn_0: 0.7769 (0.8932)  loss_bbox_dn_1: 0.7729 (0.8978)  loss_bbox_dn_2: 0.7702 (0.9032)  loss_bbox_dn_3: 0.7685 (0.9074)  loss_bbox_dn_4: 0.7666 (0.9105)  loss_bbox_dn_5: 0.7653 (0.9139)  loss_giou: 1.3844 (1.4312)  loss_giou_aux_0: 1.4158 (1.4614)  loss_giou_aux_1: 1.4045 (1.4477)  loss_giou_aux_2: 1.3932 (1.4426)  loss_giou_aux_3: 1.3902 (1.4384)  loss_giou_aux_4: 1.3786 (1.4328)  loss_giou_aux_5: 1.4186 (1.4874)  loss_giou_dn_0: 1.3438 (1.3297)  loss_giou_dn_1: 1.3391 (1.3324)  loss_giou_dn_2: 1.3432 (1.3379)  loss_giou_dn_3: 1.3481 (1.3435)  loss_giou_dn_4: 1.3569 (1.3508)  loss_giou_dn_5: 1.3601 (1.3586)  loss_vfl: 0.6772 (0.6921)  loss_vfl_aux_0: 0.6303 (0.6264)  loss_vfl_aux_1: 0.6374 (0.6562)  loss_vfl_aux_2: 0.6660 (0.6769)  loss_vfl_aux_3: 0.6715 (0.6819)  loss_vfl_aux_4: 0.6840 (0.6890)  loss_vfl_aux_5: 0.6041 (0.5905)  loss_vfl_dn_0: 0.5085 (0.6025)  loss_vfl_dn_1: 0.4938 (0.5873)  loss_vfl_dn_2: 0.5080 (0.5947)  loss_vfl_dn_3: 0.5104 (0.5921)  loss_vfl_dn_4: 0.5017 (0.5940)  loss_vfl_dn_5: 0.5032 (0.5887)  time: 0.2840  data: 0.0111  max mem: 5650
Epoch: [0]  [  200/14786]  eta: 1:09:49  lr: 0.000010  loss: 34.0323 (38.0167)  loss_bbox: 0.5264 (0.9051)  loss_bbox_aux_0: 0.5509 (0.9596)  loss_bbox_aux_1: 0.5404 (0.9338)  loss_bbox_aux_2: 0.5248 (0.9208)  loss_bbox_aux_3: 0.5305 (0.9140)  loss_bbox_aux_4: 0.5308 (0.9086)  loss_bbox_aux_5: 0.6128 (1.0113)  loss_bbox_dn_0: 0.7522 (0.8707)  loss_bbox_dn_1: 0.7444 (0.8689)  loss_bbox_dn_2: 0.7424 (0.8698)  loss_bbox_dn_3: 0.7416 (0.8714)  loss_bbox_dn_4: 0.7418 (0.8716)  loss_bbox_dn_5: 0.7416 (0.8730)  loss_giou: 1.1894 (1.3372)  loss_giou_aux_0: 1.2174 (1.3749)  loss_giou_aux_1: 1.2085 (1.3574)  loss_giou_aux_2: 1.1974 (1.3491)  loss_giou_aux_3: 1.1865 (1.3435)  loss_giou_aux_4: 1.1864 (1.3392)  loss_giou_aux_5: 1.2920 (1.4172)  loss_giou_dn_0: 1.3025 (1.3231)  loss_giou_dn_1: 1.2891 (1.3223)  loss_giou_dn_2: 1.2973 (1.3266)  loss_giou_dn_3: 1.2954 (1.3315)  loss_giou_dn_4: 1.3036 (1.3381)  loss_giou_dn_5: 1.3077 (1.3443)  loss_vfl: 0.8750 (0.8169)  loss_vfl_aux_0: 0.7475 (0.7283)  loss_vfl_aux_1: 0.7961 (0.7629)  loss_vfl_aux_2: 0.8587 (0.7883)  loss_vfl_aux_3: 0.8222 (0.7994)  loss_vfl_aux_4: 0.8579 (0.8114)  loss_vfl_aux_5: 0.7301 (0.6796)  loss_vfl_dn_0: 0.4669 (0.5501)  loss_vfl_dn_1: 0.4986 (0.5477)  loss_vfl_dn_2: 0.5219 (0.5595)  loss_vfl_dn_3: 0.5205 (0.5618)  loss_vfl_dn_4: 0.5341 (0.5657)  loss_vfl_dn_5: 0.5277 (0.5621)  time: 0.2889  data: 0.0113  max mem: 5652
