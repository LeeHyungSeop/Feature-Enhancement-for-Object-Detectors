WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 0): env://
| distributed init (rank 1): env://
Namespace(data_path='/media/data/coco2017', dataset='coco', model='my_retinanet_resnet50_fpn', device='cuda', batch_size=2, epochs=26, workers=4, opt='sgd', lr=0.005, momentum=0.9, weight_decay=0.0001, norm_weight_decay=None, lr_scheduler='multisteplr', lr_step_size=8, lr_steps=[16, 22], lr_gamma=0.1, print_freq=20, output_dir='./outputs/RetinaNet/baseline', resume='', start_epoch=0, aspect_ratio_group_factor=3, rpn_score_thresh=None, trainable_backbone_layers=None, data_augmentation='hflip', sync_bn=False, test_only=False, use_deterministic_algorithms=False, world_size=2, dist_url='env://', weights=None, weights_backbone='ResNet50_Weights.IMAGENET1K_V1', amp=False, use_copypaste=False, backend='pil', use_v2=False, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Loading data
root:  /media/data/coco2017
img_folder:  train2017
loading annotations into memory...
Done (t=8.60s)
creating index...
index created!
root:  /media/data/coco2017
img_folder:  val2017
loading annotations into memory...
/home/hslee/Backbone-Neck-Self-Distillation/FRCN_RetinaNet_FCOS/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Done (t=0.22s)
creating index...
index created!
Creating data loaders
Using [0, 0.5, 0.6299605249474365, 0.7937005259840997, 1.0, 1.259921049894873, 1.5874010519681991, 2.0, inf] as bins for aspect ratio quantization
Count of instances per bin: [  104   982 24236  2332  8225 74466  5763  1158]
Creating model
/home/hslee/Backbone-Neck-Self-Distillation/FRCN_RetinaNet_FCOS/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
return_layers: {'layer2': '0', 'layer3': '1', 'layer4': '2'}
extra_blocks: LastLevelP6P7(
  (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)
backbone: BackboneWithFPN(
  (body): IntermediateLayerGetter(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): FrozenBatchNorm2d(64, eps=1e-05)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(64, eps=1e-05)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(64, eps=1e-05)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(256, eps=1e-05)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): FrozenBatchNorm2d(256, eps=1e-05)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(64, eps=1e-05)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(64, eps=1e-05)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(256, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(64, eps=1e-05)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(64, eps=1e-05)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(256, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(128, eps=1e-05)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(128, eps=1e-05)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(512, eps=1e-05)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d(512, eps=1e-05)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(128, eps=1e-05)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(128, eps=1e-05)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(512, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(128, eps=1e-05)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(128, eps=1e-05)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(512, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(128, eps=1e-05)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(128, eps=1e-05)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(512, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(256, eps=1e-05)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(256, eps=1e-05)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d(1024, eps=1e-05)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(256, eps=1e-05)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(256, eps=1e-05)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(256, eps=1e-05)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(256, eps=1e-05)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(256, eps=1e-05)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(256, eps=1e-05)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(256, eps=1e-05)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(256, eps=1e-05)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(256, eps=1e-05)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(256, eps=1e-05)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(512, eps=1e-05)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(512, eps=1e-05)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d(2048, eps=1e-05)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(512, eps=1e-05)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(512, eps=1e-05)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(512, eps=1e-05)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(512, eps=1e-05)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (fpn): FeaturePyramidNetwork(
    (inner_blocks): ModuleList(
      (0): Conv2dNormActivation(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): Conv2dNormActivation(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (layer_blocks): ModuleList(
      (0-2): 3 x Conv2dNormActivation(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (extra_blocks): LastLevelP6P7(
      (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
)
Start training
Epoch: [0]  [    0/29316]  eta: 10:07:54  lr: 0.000010  loss: 3.0204 (3.0204)  bbox_regression: 0.6828 (0.6828)  classification: 1.3888 (1.3888)  loss_nb_mse: 0.9488 (0.9488)  time: 1.2442  data: 0.3854  max mem: 3081
Epoch: [0]  [   20/29316]  eta: 1:35:55  lr: 0.000110  loss: 2.7960 (2.8675)  bbox_regression: 0.6823 (0.7247)  classification: 1.2879 (1.3086)  loss_nb_mse: 0.7943 (0.8342)  time: 0.1441  data: 0.0037  max mem: 3747
Epoch: [0]  [   40/29316]  eta: 1:23:34  lr: 0.000210  loss: 2.2360 (2.5877)  bbox_regression: 0.6781 (0.7059)  classification: 1.2747 (1.3057)  loss_nb_mse: 0.2776 (0.5761)  time: 0.1449  data: 0.0036  max mem: 3747
Epoch: [0]  [   60/29316]  eta: 1:19:31  lr: 0.000310  loss: 2.1071 (2.4543)  bbox_regression: 0.6866 (0.7227)  classification: 1.2619 (1.2959)  loss_nb_mse: 0.1430 (0.4357)  time: 0.1463  data: 0.0037  max mem: 3747
Epoch: [0]  [   80/29316]  eta: 1:17:18  lr: 0.000410  loss: 2.0124 (2.3574)  bbox_regression: 0.6780 (0.7154)  classification: 1.2412 (1.2913)  loss_nb_mse: 0.0853 (0.3507)  time: 0.1451  data: 0.0037  max mem: 3747
Epoch: [0]  [  100/29316]  eta: 1:15:55  lr: 0.000509  loss: 2.0416 (2.3011)  bbox_regression: 0.6930 (0.7214)  classification: 1.2705 (1.2862)  loss_nb_mse: 0.0600 (0.2935)  time: 0.1448  data: 0.0037  max mem: 3747
Epoch: [0]  [  120/29316]  eta: 1:15:06  lr: 0.000609  loss: 1.9939 (2.2650)  bbox_regression: 0.6934 (0.7287)  classification: 1.2422 (1.2841)  loss_nb_mse: 0.0431 (0.2522)  time: 0.1466  data: 0.0037  max mem: 3747
Epoch: [0]  [  140/29316]  eta: 1:14:35  lr: 0.000709  loss: 1.9928 (2.2310)  bbox_regression: 0.6610 (0.7229)  classification: 1.2734 (1.2874)  loss_nb_mse: 0.0314 (0.2208)  time: 0.1476  data: 0.0035  max mem: 3747
Epoch: [0]  [  160/29316]  eta: 1:14:17  lr: 0.000809  loss: 1.9612 (2.2021)  bbox_regression: 0.6647 (0.7192)  classification: 1.2640 (1.2867)  loss_nb_mse: 0.0233 (0.1963)  time: 0.1491  data: 0.0037  max mem: 3747
Epoch: [0]  [  180/29316]  eta: 1:13:49  lr: 0.000909  loss: 1.9600 (2.1818)  bbox_regression: 0.6706 (0.7152)  classification: 1.2709 (1.2900)  loss_nb_mse: 0.0176 (0.1765)  time: 0.1452  data: 0.0036  max mem: 3747
Epoch: [0]  [  200/29316]  eta: 1:13:30  lr: 0.001009  loss: 1.9858 (2.1618)  bbox_regression: 0.6576 (0.7115)  classification: 1.2628 (1.2898)  loss_nb_mse: 0.0146 (0.1605)  time: 0.1466  data: 0.0033  max mem: 3747
Epoch: [0]  [  220/29316]  eta: 1:13:20  lr: 0.001109  loss: 1.9298 (2.1439)  bbox_regression: 0.6760 (0.7097)  classification: 1.2383 (1.2871)  loss_nb_mse: 0.0121 (0.1470)  time: 0.1488  data: 0.0037  max mem: 3747
Epoch: [0]  [  240/29316]  eta: 1:13:06  lr: 0.001209  loss: 1.9803 (2.1328)  bbox_regression: 0.6796 (0.7113)  classification: 1.2796 (1.2857)  loss_nb_mse: 0.0108 (0.1358)  time: 0.1468  data: 0.0038  max mem: 3747
Epoch: [0]  [  260/29316]  eta: 1:13:01  lr: 0.001309  loss: 1.9919 (2.1242)  bbox_regression: 0.6886 (0.7105)  classification: 1.3079 (1.2876)  loss_nb_mse: 0.0096 (0.1261)  time: 0.1499  data: 0.0036  max mem: 3747
Epoch: [0]  [  280/29316]  eta: 1:12:51  lr: 0.001409  loss: 1.9714 (2.1149)  bbox_regression: 0.6644 (0.7082)  classification: 1.2550 (1.2889)  loss_nb_mse: 0.0085 (0.1177)  time: 0.1473  data: 0.0037  max mem: 3747
Epoch: [0]  [  300/29316]  eta: 1:12:42  lr: 0.001508  loss: 1.9934 (2.1071)  bbox_regression: 0.6807 (0.7093)  classification: 1.2725 (1.2873)  loss_nb_mse: 0.0077 (0.1104)  time: 0.1475  data: 0.0036  max mem: 3747
Epoch: [0]  [  320/29316]  eta: 1:12:35  lr: 0.001608  loss: 1.9584 (2.1004)  bbox_regression: 0.6709 (0.7090)  classification: 1.2716 (1.2874)  loss_nb_mse: 0.0069 (0.1040)  time: 0.1480  data: 0.0036  max mem: 3747
Epoch: [0]  [  340/29316]  eta: 1:12:30  lr: 0.001708  loss: 1.9278 (2.0915)  bbox_regression: 0.6594 (0.7066)  classification: 1.2558 (1.2866)  loss_nb_mse: 0.0060 (0.0982)  time: 0.1495  data: 0.0037  max mem: 3747
Epoch: [0]  [  360/29316]  eta: 1:12:28  lr: 0.001808  loss: 1.9330 (2.0860)  bbox_regression: 0.6757 (0.7066)  classification: 1.2595 (1.2863)  loss_nb_mse: 0.0054 (0.0931)  time: 0.1505  data: 0.0036  max mem: 3747
Epoch: [0]  [  380/29316]  eta: 1:12:22  lr: 0.001908  loss: 1.9127 (2.0789)  bbox_regression: 0.6574 (0.7049)  classification: 1.2684 (1.2855)  loss_nb_mse: 0.0047 (0.0885)  time: 0.1481  data: 0.0037  max mem: 3747
Epoch: [0]  [  400/29316]  eta: 1:12:21  lr: 0.002008  loss: 1.9368 (2.0737)  bbox_regression: 0.6893 (0.7050)  classification: 1.2442 (1.2844)  loss_nb_mse: 0.0044 (0.0843)  time: 0.1518  data: 0.0037  max mem: 3747
Epoch: [0]  [  420/29316]  eta: 1:12:22  lr: 0.002108  loss: 1.9724 (2.0716)  bbox_regression: 0.6747 (0.7057)  classification: 1.2880 (1.2854)  loss_nb_mse: 0.0039 (0.0805)  time: 0.1528  data: 0.0035  max mem: 3747
Epoch: [0]  [  440/29316]  eta: 1:12:20  lr: 0.002208  loss: 1.9284 (2.0661)  bbox_regression: 0.6794 (0.7051)  classification: 1.2483 (1.2840)  loss_nb_mse: 0.0036 (0.0770)  time: 0.1511  data: 0.0037  max mem: 3747
Epoch: [0]  [  460/29316]  eta: 1:12:19  lr: 0.002308  loss: 1.9537 (2.0624)  bbox_regression: 0.6566 (0.7043)  classification: 1.2854 (1.2844)  loss_nb_mse: 0.0033 (0.0738)  time: 0.1515  data: 0.0036  max mem: 3747
Epoch: [0]  [  480/29316]  eta: 1:12:17  lr: 0.002408  loss: 1.9434 (2.0594)  bbox_regression: 0.6686 (0.7050)  classification: 1.2433 (1.2837)  loss_nb_mse: 0.0028 (0.0708)  time: 0.1518  data: 0.0039  max mem: 3747
Epoch: [0]  [  500/29316]  eta: 1:12:13  lr: 0.002507  loss: 1.9399 (2.0555)  bbox_regression: 0.6730 (0.7043)  classification: 1.2541 (1.2831)  loss_nb_mse: 0.0026 (0.0681)  time: 0.1495  data: 0.0037  max mem: 3747
Epoch: [0]  [  520/29316]  eta: 1:12:12  lr: 0.002607  loss: 1.9433 (2.0529)  bbox_regression: 0.6806 (0.7046)  classification: 1.2613 (1.2827)  loss_nb_mse: 0.0023 (0.0656)  time: 0.1517  data: 0.0035  max mem: 3747
Epoch: [0]  [  540/29316]  eta: 1:12:11  lr: 0.002707  loss: 1.9184 (2.0491)  bbox_regression: 0.6779 (0.7039)  classification: 1.2447 (1.2820)  loss_nb_mse: 0.0021 (0.0632)  time: 0.1524  data: 0.0037  max mem: 3747
Epoch: [0]  [  560/29316]  eta: 1:12:09  lr: 0.002807  loss: 1.9547 (2.0464)  bbox_regression: 0.6710 (0.7036)  classification: 1.2607 (1.2817)  loss_nb_mse: 0.0019 (0.0611)  time: 0.1519  data: 0.0038  max mem: 3747
Epoch: [0]  [  580/29316]  eta: 1:12:05  lr: 0.002907  loss: 1.9394 (2.0435)  bbox_regression: 0.6614 (0.7025)  classification: 1.2811 (1.2820)  loss_nb_mse: 0.0017 (0.0590)  time: 0.1495  data: 0.0035  max mem: 3747
Epoch: [0]  [  600/29316]  eta: 1:11:58  lr: 0.003007  loss: 1.9987 (2.0417)  bbox_regression: 0.7031 (0.7030)  classification: 1.2714 (1.2816)  loss_nb_mse: 0.0015 (0.0571)  time: 0.1464  data: 0.0039  max mem: 3747
Epoch: [0]  [  620/29316]  eta: 1:11:57  lr: 0.003107  loss: 1.9594 (2.0402)  bbox_regression: 0.6604 (0.7026)  classification: 1.2794 (1.2823)  loss_nb_mse: 0.0014 (0.0553)  time: 0.1526  data: 0.0035  max mem: 3747
Epoch: [0]  [  640/29316]  eta: 1:11:58  lr: 0.003207  loss: 1.9056 (2.0365)  bbox_regression: 0.6640 (0.7019)  classification: 1.2153 (1.2809)  loss_nb_mse: 0.0013 (0.0536)  time: 0.1541  data: 0.0037  max mem: 3747
Epoch: [0]  [  660/29316]  eta: 1:11:56  lr: 0.003307  loss: 1.9520 (2.0345)  bbox_regression: 0.6637 (0.7014)  classification: 1.2823 (1.2811)  loss_nb_mse: 0.0012 (0.0520)  time: 0.1522  data: 0.0035  max mem: 3747
Epoch: [0]  [  680/29316]  eta: 1:11:54  lr: 0.003407  loss: 1.9444 (2.0327)  bbox_regression: 0.6702 (0.7010)  classification: 1.2450 (1.2811)  loss_nb_mse: 0.0011 (0.0505)  time: 0.1513  data: 0.0037  max mem: 3747
Epoch: [0]  [  700/29316]  eta: 1:11:52  lr: 0.003506  loss: 1.9382 (2.0302)  bbox_regression: 0.6725 (0.7003)  classification: 1.2790 (1.2808)  loss_nb_mse: 0.0010 (0.0491)  time: 0.1527  data: 0.0037  max mem: 3747
Epoch: [0]  [  720/29316]  eta: 1:11:52  lr: 0.003606  loss: 1.9275 (2.0276)  bbox_regression: 0.6506 (0.6993)  classification: 1.2573 (1.2805)  loss_nb_mse: 0.0010 (0.0478)  time: 0.1547  data: 0.0038  max mem: 3747
Epoch: [0]  [  740/29316]  eta: 1:11:51  lr: 0.003706  loss: 1.9284 (2.0256)  bbox_regression: 0.6583 (0.6987)  classification: 1.2603 (1.2804)  loss_nb_mse: 0.0008 (0.0465)  time: 0.1528  data: 0.0036  max mem: 3747
Epoch: [0]  [  760/29316]  eta: 1:11:48  lr: 0.003806  loss: 1.8893 (2.0235)  bbox_regression: 0.6736 (0.6985)  classification: 1.2137 (1.2797)  loss_nb_mse: 0.0008 (0.0453)  time: 0.1517  data: 0.0037  max mem: 3747
Epoch: [0]  [  780/29316]  eta: 1:11:48  lr: 0.003906  loss: 1.9694 (2.0241)  bbox_regression: 0.6716 (0.6999)  classification: 1.2398 (1.2801)  loss_nb_mse: 0.0007 (0.0442)  time: 0.1542  data: 0.0037  max mem: 3747
Epoch: [0]  [  800/29316]  eta: 1:11:46  lr: 0.004006  loss: 1.9349 (2.0233)  bbox_regression: 0.6717 (0.6997)  classification: 1.2626 (1.2804)  loss_nb_mse: 0.0007 (0.0431)  time: 0.1527  data: 0.0036  max mem: 3747
Epoch: [0]  [  820/29316]  eta: 1:11:44  lr: 0.004106  loss: 1.9387 (2.0219)  bbox_regression: 0.6712 (0.6996)  classification: 1.2267 (1.2802)  loss_nb_mse: 0.0006 (0.0421)  time: 0.1526  data: 0.0038  max mem: 3747
Epoch: [0]  [  840/29316]  eta: 1:11:43  lr: 0.004206  loss: 1.9091 (2.0198)  bbox_regression: 0.6625 (0.6995)  classification: 1.2190 (1.2793)  loss_nb_mse: 0.0006 (0.0411)  time: 0.1533  data: 0.0037  max mem: 3747
Epoch: [0]  [  860/29316]  eta: 1:11:42  lr: 0.004306  loss: 1.9491 (2.0187)  bbox_regression: 0.6805 (0.6995)  classification: 1.2328 (1.2790)  loss_nb_mse: 0.0006 (0.0401)  time: 0.1551  data: 0.0035  max mem: 3747
Epoch: [0]  [  880/29316]  eta: 1:11:39  lr: 0.004406  loss: 1.9031 (2.0171)  bbox_regression: 0.6716 (0.6996)  classification: 1.2199 (1.2783)  loss_nb_mse: 0.0005 (0.0392)  time: 0.1507  data: 0.0037  max mem: 3747
Epoch: [0]  [  900/29316]  eta: 1:11:37  lr: 0.004505  loss: 1.8996 (2.0154)  bbox_regression: 0.6540 (0.6990)  classification: 1.2236 (1.2780)  loss_nb_mse: 0.0005 (0.0384)  time: 0.1536  data: 0.0035  max mem: 3747
Epoch: [0]  [  920/29316]  eta: 1:11:36  lr: 0.004605  loss: 1.8738 (2.0126)  bbox_regression: 0.6523 (0.6980)  classification: 1.2392 (1.2770)  loss_nb_mse: 0.0005 (0.0376)  time: 0.1532  data: 0.0035  max mem: 3747
Epoch: [0]  [  940/29316]  eta: 1:11:35  lr: 0.004705  loss: 1.8968 (2.0112)  bbox_regression: 0.6747 (0.6979)  classification: 1.2206 (1.2765)  loss_nb_mse: 0.0004 (0.0368)  time: 0.1556  data: 0.0038  max mem: 3747
Epoch: [0]  [  960/29316]  eta: 1:11:34  lr: 0.004805  loss: 1.8829 (2.0094)  bbox_regression: 0.6608 (0.6974)  classification: 1.2170 (1.2760)  loss_nb_mse: 0.0004 (0.0360)  time: 0.1547  data: 0.0036  max mem: 3747
Epoch: [0]  [  980/29316]  eta: 1:11:32  lr: 0.004905  loss: 2.0248 (2.0093)  bbox_regression: 0.6904 (0.6977)  classification: 1.2765 (1.2763)  loss_nb_mse: 0.0004 (0.0353)  time: 0.1526  data: 0.0033  max mem: 3747
Epoch: [0]  [ 1000/29316]  eta: 1:11:30  lr: 0.005000  loss: 1.9020 (2.0074)  bbox_regression: 0.6495 (0.6972)  classification: 1.2251 (1.2756)  loss_nb_mse: 0.0003 (0.0346)  time: 0.1546  data: 0.0037  max mem: 3747
Epoch: [0]  [ 1020/29316]  eta: 1:11:30  lr: 0.005000  loss: 1.8568 (2.0052)  bbox_regression: 0.6631 (0.6967)  classification: 1.2230 (1.2745)  loss_nb_mse: 0.0003 (0.0339)  time: 0.1559  data: 0.0037  max mem: 3747
Epoch: [0]  [ 1040/29316]  eta: 1:11:28  lr: 0.005000  loss: 1.8900 (2.0031)  bbox_regression: 0.6630 (0.6965)  classification: 1.2190 (1.2733)  loss_nb_mse: 0.0003 (0.0333)  time: 0.1542  data: 0.0037  max mem: 3747
Epoch: [0]  [ 1060/29316]  eta: 1:11:27  lr: 0.005000  loss: 1.9195 (2.0027)  bbox_regression: 0.6839 (0.6966)  classification: 1.2352 (1.2735)  loss_nb_mse: 0.0003 (0.0327)  time: 0.1559  data: 0.0035  max mem: 3747
Epoch: [0]  [ 1080/29316]  eta: 1:11:26  lr: 0.005000  loss: 1.8727 (2.0006)  bbox_regression: 0.6658 (0.6963)  classification: 1.1888 (1.2722)  loss_nb_mse: 0.0003 (0.0321)  time: 0.1545  data: 0.0035  max mem: 3747
Epoch: [0]  [ 1100/29316]  eta: 1:11:25  lr: 0.005000  loss: 1.8627 (1.9998)  bbox_regression: 0.6498 (0.6962)  classification: 1.2271 (1.2721)  loss_nb_mse: 0.0003 (0.0315)  time: 0.1560  data: 0.0036  max mem: 3747
Epoch: [0]  [ 1120/29316]  eta: 1:11:22  lr: 0.005000  loss: 1.8951 (1.9987)  bbox_regression: 0.6740 (0.6963)  classification: 1.1999 (1.2715)  loss_nb_mse: 0.0003 (0.0309)  time: 0.1528  data: 0.0039  max mem: 3747
Epoch: [0]  [ 1140/29316]  eta: 1:11:21  lr: 0.005000  loss: 1.8700 (1.9972)  bbox_regression: 0.6669 (0.6963)  classification: 1.1988 (1.2705)  loss_nb_mse: 0.0002 (0.0304)  time: 0.1560  data: 0.0037  max mem: 3747
Epoch: [0]  [ 1160/29316]  eta: 1:11:18  lr: 0.005000  loss: 1.8710 (1.9960)  bbox_regression: 0.6564 (0.6963)  classification: 1.2139 (1.2698)  loss_nb_mse: 0.0002 (0.0299)  time: 0.1506  data: 0.0035  max mem: 3747
Epoch: [0]  [ 1180/29316]  eta: 1:11:13  lr: 0.005000  loss: 1.8668 (1.9939)  bbox_regression: 0.6664 (0.6959)  classification: 1.1676 (1.2686)  loss_nb_mse: 0.0002 (0.0294)  time: 0.1488  data: 0.0038  max mem: 3747
Epoch: [0]  [ 1200/29316]  eta: 1:11:11  lr: 0.005000  loss: 1.8816 (1.9922)  bbox_regression: 0.6518 (0.6955)  classification: 1.2100 (1.2678)  loss_nb_mse: 0.0002 (0.0289)  time: 0.1546  data: 0.0037  max mem: 3747
Epoch: [0]  [ 1220/29316]  eta: 1:11:10  lr: 0.005000  loss: 1.9245 (1.9918)  bbox_regression: 0.6793 (0.6958)  classification: 1.2236 (1.2675)  loss_nb_mse: 0.0002 (0.0284)  time: 0.1547  data: 0.0036  max mem: 3747
Epoch: [0]  [ 1240/29316]  eta: 1:11:07  lr: 0.005000  loss: 1.8333 (1.9896)  bbox_regression: 0.6609 (0.6956)  classification: 1.1663 (1.2661)  loss_nb_mse: 0.0002 (0.0280)  time: 0.1530  data: 0.0036  max mem: 3747
Epoch: [0]  [ 1260/29316]  eta: 1:11:05  lr: 0.005000  loss: 1.8439 (1.9878)  bbox_regression: 0.6796 (0.6955)  classification: 1.1639 (1.2648)  loss_nb_mse: 0.0002 (0.0275)  time: 0.1548  data: 0.0039  max mem: 3747
Epoch: [0]  [ 1280/29316]  eta: 1:11:02  lr: 0.005000  loss: 1.9072 (1.9869)  bbox_regression: 0.6708 (0.6958)  classification: 1.1788 (1.2641)  loss_nb_mse: 0.0003 (0.0271)  time: 0.1524  data: 0.0037  max mem: 3747
Epoch: [0]  [ 1300/29316]  eta: 1:11:00  lr: 0.005000  loss: 1.9239 (1.9865)  bbox_regression: 0.6680 (0.6963)  classification: 1.1866 (1.2634)  loss_nb_mse: 0.0004 (0.0267)  time: 0.1528  data: 0.0037  max mem: 3747
Epoch: [0]  [ 1320/29316]  eta: 1:10:58  lr: 0.005000  loss: 1.8011 (1.9845)  bbox_regression: 0.6706 (0.6960)  classification: 1.1206 (1.2622)  loss_nb_mse: 0.0006 (0.0263)  time: 0.1542  data: 0.0037  max mem: 3747
Epoch: [0]  [ 1340/29316]  eta: 1:10:55  lr: 0.005000  loss: 1.8520 (1.9828)  bbox_regression: 0.6719 (0.6961)  classification: 1.1371 (1.2608)  loss_nb_mse: 0.0008 (0.0259)  time: 0.1545  data: 0.0037  max mem: 3747
Epoch: [0]  [ 1360/29316]  eta: 1:10:53  lr: 0.005000  loss: 1.8639 (1.9812)  bbox_regression: 0.7045 (0.6963)  classification: 1.1599 (1.2594)  loss_nb_mse: 0.0012 (0.0255)  time: 0.1535  data: 0.0039  max mem: 3747
